== Module 6: Red Hat OpenShift AI
:navtitle: AI Model Serving and Development with Red Hat OpenShift AI

This module includes two hands-on exercises:

* Interact with a Retrieval-Augmented Generation (RAG) application deployed on OpenShift using vLLM as the inference server.
* Explore an agentic workflow in a Jupyter notebook using OpenShift AI Workbenches to generate finance articles by searching the web for relevant information.

== Exercise 1: RAG Application with vLLM Inference Server

. Access the RAG demo application at the following URL:
https://prod-streamlit-finance-agent.apps.cluster-922vm.922vm.sandbox3207.opentlc.com/
+
. Review the set of articles used to create the grounding dataset.
. Ask the chatbot questions related to the grounding documents.
. Observe the response times and overall performance.

== Exercise 2: Agentic AI Workflow in OpenShift AI Workbenches

. Login to Red Hat OpenShift AI at the following URL:  
https://rhods-dashboard-redhat-ods-applications.apps.cluster-922vm.922vm.sandbox3207.opentlc.com/

. Authenticate using your personal login credentials provided in the spreadsheet.

. Access your personal OpenShift AI Workbench:
  * Click *Data Science Projects* from the left navigation menu.
  * Select the project that corresponds to your username.
  * Click the *Workbenches* tab in the upper navigation menu.
  * Locate the `agentic-workbench` and click **Open** on the far right.

. Walk through the notebook:
  * Read the cell descriptions to understand the application's purpose and flow.
  * Run each cell and review the outputs.

. In the final cell:
  * Review the list of articles retrieved by the agent.
  * Examine how this information is used to generate a finance article.

. Once you've completed the workflow:
  * Try modifying parts of the code (e.g., prompt templates, model temperature, and prompt query).
  * Observe how these changes impact the generated output.


== Takeaways

* Using vLLM as an inference server results in faster response times, reduced hardware costs, and greater scalability.
* Self-hosting LLMs with vLLM gives teams greater control over data privacy and usage compared to using a third-party LLM service or application.
* OpenShift AI provides a powerful and consistent AI/ML platform for data science and development teams to build and deploy intelligent applications.
* Built on OpenShift, OpenShift AI accelerates the delivery of AI applications from proof of concept to production.

